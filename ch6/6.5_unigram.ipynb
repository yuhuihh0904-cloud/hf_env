{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd1e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b30a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596a5872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'▁This': 3,\n",
       "             '▁is': 2,\n",
       "             '▁the': 1,\n",
       "             '▁Hugging': 1,\n",
       "             '▁Face': 1,\n",
       "             '▁Course.': 1,\n",
       "             '▁chapter': 1,\n",
       "             '▁about': 1,\n",
       "             '▁tokenization.': 1,\n",
       "             '▁section': 1,\n",
       "             '▁shows': 1,\n",
       "             '▁several': 1,\n",
       "             '▁tokenizer': 1,\n",
       "             '▁algorithms.': 1,\n",
       "             '▁Hopefully,': 1,\n",
       "             '▁you': 1,\n",
       "             '▁will': 1,\n",
       "             '▁be': 1,\n",
       "             '▁able': 1,\n",
       "             '▁to': 1,\n",
       "             '▁understand': 1,\n",
       "             '▁how': 1,\n",
       "             '▁they': 1,\n",
       "             '▁are': 1,\n",
       "             '▁trained': 1,\n",
       "             '▁and': 1,\n",
       "             '▁generate': 1,\n",
       "             '▁tokens.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "     word_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "     new_words = [word for word,_ in word_with_offsets]\n",
    "     for word in new_words:\n",
    "          word_freqs[word] +=1\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e3f9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁t', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('▁a', 5),\n",
       " ('▁to', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('▁T', 3),\n",
       " ('▁Th', 3),\n",
       " ('▁Thi', 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "     for i in range(len(word)):\n",
    "          char_freqs[word[i]] += freq\n",
    "          #双重循环获得子词\n",
    "          for j in range(i +2,len(word)+1):\n",
    "               subwords_freqs[word[i:j]] += freq\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key = lambda x : x[1], reverse=True)\n",
    "#.items()会将字典转换为列表\n",
    "sorted_subwords[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d55e3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁': 31,\n",
       " 'T': 3,\n",
       " 'h': 9,\n",
       " 'i': 13,\n",
       " 's': 13,\n",
       " 't': 14,\n",
       " 'e': 21,\n",
       " 'H': 2,\n",
       " 'u': 6,\n",
       " 'g': 5,\n",
       " 'n': 11,\n",
       " 'F': 1,\n",
       " 'a': 12,\n",
       " 'c': 3,\n",
       " 'C': 1,\n",
       " 'o': 13,\n",
       " 'r': 9,\n",
       " '.': 4,\n",
       " 'p': 2,\n",
       " 'b': 3,\n",
       " 'k': 3,\n",
       " 'z': 2,\n",
       " 'w': 3,\n",
       " 'v': 1,\n",
       " 'l': 7,\n",
       " 'm': 1,\n",
       " 'f': 1,\n",
       " 'y': 3,\n",
       " ',': 1,\n",
       " 'd': 4,\n",
       " '▁t': 7,\n",
       " 'is': 5,\n",
       " 'er': 5,\n",
       " '▁a': 5,\n",
       " '▁to': 4,\n",
       " 'to': 4,\n",
       " 'en': 4,\n",
       " '▁T': 3,\n",
       " '▁Th': 3,\n",
       " '▁Thi': 3,\n",
       " '▁This': 3,\n",
       " 'Th': 3,\n",
       " 'Thi': 3,\n",
       " 'This': 3,\n",
       " 'hi': 3,\n",
       " 'his': 3,\n",
       " 'th': 3,\n",
       " 'ou': 3,\n",
       " 'se': 3,\n",
       " '▁tok': 3,\n",
       " '▁toke': 3,\n",
       " '▁token': 3,\n",
       " 'tok': 3,\n",
       " 'toke': 3,\n",
       " 'token': 3,\n",
       " 'ok': 3,\n",
       " 'oke': 3,\n",
       " 'oken': 3,\n",
       " 'ke': 3,\n",
       " 'ken': 3,\n",
       " '▁s': 3,\n",
       " 'ra': 3,\n",
       " 'nd': 3,\n",
       " '▁i': 2,\n",
       " '▁is': 2,\n",
       " '▁th': 2,\n",
       " '▁the': 2,\n",
       " 'the': 2,\n",
       " 'he': 2,\n",
       " '▁H': 2,\n",
       " 'in': 2,\n",
       " 'rs': 2,\n",
       " 'te': 2,\n",
       " '▁ab': 2,\n",
       " 'ab': 2,\n",
       " '▁tokeni': 2,\n",
       " '▁tokeniz': 2,\n",
       " 'tokeni': 2,\n",
       " 'tokeniz': 2,\n",
       " 'okeni': 2,\n",
       " 'okeniz': 2,\n",
       " 'keni': 2,\n",
       " 'keniz': 2,\n",
       " 'eni': 2,\n",
       " 'eniz': 2,\n",
       " 'ni': 2,\n",
       " 'niz': 2,\n",
       " 'iz': 2,\n",
       " 'at': 2,\n",
       " 'ti': 2,\n",
       " 'tio': 2,\n",
       " 'tion': 2,\n",
       " 'io': 2,\n",
       " 'ion': 2,\n",
       " 'on': 2,\n",
       " '▁se': 2,\n",
       " 'ho': 2,\n",
       " 'how': 2,\n",
       " 'ow': 2,\n",
       " 'era': 2,\n",
       " 'al': 2,\n",
       " 's.': 2,\n",
       " 'll': 2,\n",
       " 'an': 2,\n",
       " 'and': 2,\n",
       " 'ne': 2,\n",
       " '▁Hu': 1,\n",
       " '▁Hug': 1,\n",
       " '▁Hugg': 1,\n",
       " '▁Huggi': 1,\n",
       " '▁Huggin': 1,\n",
       " '▁Hugging': 1,\n",
       " 'Hu': 1,\n",
       " 'Hug': 1,\n",
       " 'Hugg': 1,\n",
       " 'Huggi': 1,\n",
       " 'Huggin': 1,\n",
       " 'Hugging': 1,\n",
       " 'ug': 1,\n",
       " 'ugg': 1,\n",
       " 'uggi': 1,\n",
       " 'uggin': 1,\n",
       " 'ugging': 1,\n",
       " 'gg': 1,\n",
       " 'ggi': 1,\n",
       " 'ggin': 1,\n",
       " 'gging': 1,\n",
       " 'gi': 1,\n",
       " 'gin': 1,\n",
       " 'ging': 1,\n",
       " 'ing': 1,\n",
       " 'ng': 1,\n",
       " '▁F': 1,\n",
       " '▁Fa': 1,\n",
       " '▁Fac': 1,\n",
       " '▁Face': 1,\n",
       " 'Fa': 1,\n",
       " 'Fac': 1,\n",
       " 'Face': 1,\n",
       " 'ac': 1,\n",
       " 'ace': 1,\n",
       " 'ce': 1,\n",
       " '▁C': 1,\n",
       " '▁Co': 1,\n",
       " '▁Cou': 1,\n",
       " '▁Cour': 1,\n",
       " '▁Cours': 1,\n",
       " '▁Course': 1,\n",
       " '▁Course.': 1,\n",
       " 'Co': 1,\n",
       " 'Cou': 1,\n",
       " 'Cour': 1,\n",
       " 'Cours': 1,\n",
       " 'Course': 1,\n",
       " 'Course.': 1,\n",
       " 'our': 1,\n",
       " 'ours': 1,\n",
       " 'ourse': 1,\n",
       " 'ourse.': 1,\n",
       " 'ur': 1,\n",
       " 'urs': 1,\n",
       " 'urse': 1,\n",
       " 'urse.': 1,\n",
       " 'rse': 1,\n",
       " 'rse.': 1,\n",
       " 'se.': 1,\n",
       " 'e.': 1,\n",
       " '▁c': 1,\n",
       " '▁ch': 1,\n",
       " '▁cha': 1,\n",
       " '▁chap': 1,\n",
       " '▁chapt': 1,\n",
       " '▁chapte': 1,\n",
       " '▁chapter': 1,\n",
       " 'ch': 1,\n",
       " 'cha': 1,\n",
       " 'chap': 1,\n",
       " 'chapt': 1,\n",
       " 'chapte': 1,\n",
       " 'chapter': 1,\n",
       " 'ha': 1,\n",
       " 'hap': 1,\n",
       " 'hapt': 1,\n",
       " 'hapte': 1,\n",
       " 'hapter': 1,\n",
       " 'ap': 1,\n",
       " 'apt': 1,\n",
       " 'apte': 1,\n",
       " 'apter': 1,\n",
       " 'pt': 1,\n",
       " 'pte': 1,\n",
       " 'pter': 1,\n",
       " 'ter': 1,\n",
       " '▁abo': 1,\n",
       " '▁abou': 1,\n",
       " '▁about': 1,\n",
       " 'abo': 1,\n",
       " 'abou': 1,\n",
       " 'about': 1,\n",
       " 'bo': 1,\n",
       " 'bou': 1,\n",
       " 'bout': 1,\n",
       " 'out': 1,\n",
       " 'ut': 1,\n",
       " '▁tokeniza': 1,\n",
       " '▁tokenizat': 1,\n",
       " '▁tokenizati': 1,\n",
       " '▁tokenizatio': 1,\n",
       " '▁tokenization': 1,\n",
       " '▁tokenization.': 1,\n",
       " 'tokeniza': 1,\n",
       " 'tokenizat': 1,\n",
       " 'tokenizati': 1,\n",
       " 'tokenizatio': 1,\n",
       " 'tokenization': 1,\n",
       " 'tokenization.': 1,\n",
       " 'okeniza': 1,\n",
       " 'okenizat': 1,\n",
       " 'okenizati': 1,\n",
       " 'okenizatio': 1,\n",
       " 'okenization': 1,\n",
       " 'okenization.': 1,\n",
       " 'keniza': 1,\n",
       " 'kenizat': 1,\n",
       " 'kenizati': 1,\n",
       " 'kenizatio': 1,\n",
       " 'kenization': 1,\n",
       " 'kenization.': 1,\n",
       " 'eniza': 1,\n",
       " 'enizat': 1,\n",
       " 'enizati': 1,\n",
       " 'enizatio': 1,\n",
       " 'enization': 1,\n",
       " 'enization.': 1,\n",
       " 'niza': 1,\n",
       " 'nizat': 1,\n",
       " 'nizati': 1,\n",
       " 'nizatio': 1,\n",
       " 'nization': 1,\n",
       " 'nization.': 1,\n",
       " 'iza': 1,\n",
       " 'izat': 1,\n",
       " 'izati': 1,\n",
       " 'izatio': 1,\n",
       " 'ization': 1,\n",
       " 'ization.': 1,\n",
       " 'za': 1,\n",
       " 'zat': 1,\n",
       " 'zati': 1,\n",
       " 'zatio': 1,\n",
       " 'zation': 1,\n",
       " 'zation.': 1,\n",
       " 'ati': 1,\n",
       " 'atio': 1,\n",
       " 'ation': 1,\n",
       " 'ation.': 1,\n",
       " 'tion.': 1,\n",
       " 'ion.': 1,\n",
       " 'on.': 1,\n",
       " 'n.': 1,\n",
       " '▁sec': 1,\n",
       " '▁sect': 1,\n",
       " '▁secti': 1,\n",
       " '▁sectio': 1,\n",
       " '▁section': 1,\n",
       " 'sec': 1,\n",
       " 'sect': 1,\n",
       " 'secti': 1,\n",
       " 'sectio': 1,\n",
       " 'section': 1,\n",
       " 'ec': 1,\n",
       " 'ect': 1,\n",
       " 'ecti': 1,\n",
       " 'ectio': 1,\n",
       " 'ection': 1,\n",
       " 'ct': 1,\n",
       " 'cti': 1,\n",
       " 'ctio': 1,\n",
       " 'ction': 1,\n",
       " '▁sh': 1,\n",
       " '▁sho': 1,\n",
       " '▁show': 1,\n",
       " '▁shows': 1,\n",
       " 'sh': 1,\n",
       " 'sho': 1,\n",
       " 'show': 1,\n",
       " 'shows': 1,\n",
       " 'hows': 1,\n",
       " 'ows': 1,\n",
       " 'ws': 1,\n",
       " '▁sev': 1,\n",
       " '▁seve': 1,\n",
       " '▁sever': 1,\n",
       " '▁severa': 1,\n",
       " '▁several': 1,\n",
       " 'sev': 1,\n",
       " 'seve': 1,\n",
       " 'sever': 1,\n",
       " 'severa': 1,\n",
       " 'several': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[:300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token , freq in token_freqs}\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc95c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq/ total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f570c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi算法实现\n",
    "\n",
    "def encode_word(word, model):\n",
    "     #这是一个字典的列表，每个字典都有下标序号，先按列表下标取值，再按字典取值\n",
    "     best_segmentations = [{\"start\":0,\"score\":1}]+ [\n",
    "          {\"start\":None,\"score\":None} for _ in range(len(word))\n",
    "     ]\n",
    "     for start_idx in range(len(word)):\n",
    "          best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "          for end_idx in range(start_idx + 1, len(word)+1):\n",
    "               token = word[start_idx:end_idx]\n",
    "               if token in model and best_score_at_start is not None:\n",
    "                    score = model[token] + best_score_at_start\n",
    "                    if(\n",
    "                         best_segmentations[end_idx][\"score\"] is None\n",
    "                         or best_segmentations[end_idx][\"score\"] > score\n",
    "                    ):\n",
    "                         best_segmentations[end_idx] = {\"start\":start_idx,\"score\":score}\n",
    "     segmentation = best_segmentations[-1]\n",
    "     if segmentation[\"score\"] is None:\n",
    "          return [\"<unk>\"],None\n",
    "     score = segmentation[\"score\"]\n",
    "     start = segmentation[\"start\"]\n",
    "     end = len(word)\n",
    "     tokens = []\n",
    "     while start != 0:\n",
    "          tokens.insert(0,word[start:end])\n",
    "          next_start = best_segmentations[start][\"start\"]\n",
    "          end = start\n",
    "          start = next_start\n",
    "     tokens.insert(0,word[start:end])\n",
    "     return tokens,score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19cb236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\",model))\n",
    "print(encode_word(\"This\",model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67226cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算语料库上的分词损失\n",
    "def compute_loss(model):\n",
    "     loss = 0\n",
    "     for word, freq in word_freqs.items():\n",
    "          _,word_loss = encode_word(word,model)\n",
    "          loss+= freq*word_loss\n",
    "     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691aeaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "765ccd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算每个词的分数，通过计算删除每个词得到的模型的损失\n",
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "     scores = {}\n",
    "     model_loss = compute_loss(model)\n",
    "     for token, score in model.items():\n",
    "          #保留长度为1的token\n",
    "          if len(token) == 1:\n",
    "               continue\n",
    "          model_without_token = copy.deepcopy(model)\n",
    "          _ = model_without_token.pop(token)\n",
    "          scores[token] = compute_loss(model_without_token) - model_loss\n",
    "     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f20f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afae592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了提高效率，通常，每次删除当前词表的10%\n",
    "\n",
    "# [\n",
    "#     # (Token名字,  它的得分Score)\n",
    "#     (\"apple\", 0.001),   # 索引 0 (i=0)\n",
    "#     (\"banana\", 0.005),  # 索引 1 (i=1)\n",
    "#     (\"orange\", 10.5),   # 索引 2 (i=2)\n",
    "#     ...0\n",
    "# ]\n",
    "\n",
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # 删除分数最低的percent_to_remov tokens 。\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48dbe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁the',\n",
       " '▁Hugging',\n",
       " '▁Face',\n",
       " '▁',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
