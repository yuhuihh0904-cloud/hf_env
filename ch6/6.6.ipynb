{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59182ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\",name = \"wikitext-2-raw-v1\",split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996a0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_corpus():\n",
    "     for i in range(0,len(dataset),1000):\n",
    "          yield dataset[i:i+1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee733179",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wikitext-2.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "     for i in range(len(dataset)):\n",
    "          f.write(dataset[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a38920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "#从头构建tokenizer库，首先要实例化一个带有model的tokenizer对象\n",
    "#然后normalizer-pre_tokenizer-post_processor-decoder\n",
    "\n",
    "from tokenizers import (\n",
    "     decoders,\n",
    "     models,\n",
    "     normalizers,\n",
    "     pre_tokenizers,\n",
    "     processors,\n",
    "     trainers,\n",
    "     Tokenizer,\n",
    ")\n",
    "#确定model\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token= \"[UNK]\"))\n",
    "#建立normalization\n",
    "#A:直接用hugging face打包好的\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "#B：手动组装流水线\n",
    "#NFD():unicode标准化，把重音字符拆成字符和重音标志\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "     [normalizers.NFD(),normalizers.Lowercase(),normalizers.StripAccents()]\n",
    ")\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a1cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预分词\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "# Whitespace 会使用空格和所有不是字母、数字或下划线的字符进行分割\n",
    "#whitespacesplit只使用空格分割\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5efd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d9d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization 流程的下一步是将输入数据传递给模型。\n",
    "#我们已经在初始化时指定了我们的模型，但是我们还需要对其进行训练\n",
    "special_tokens = [\"[UNK]\",\"[PAD]\",\"[CLS]\",\"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000,special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(get_train_corpus(),trainer=trainer)\n",
    "tokenizer.train([\"wikitext-2.txt\"],trainer = trainer)\n",
    "\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04aa36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id,sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a07be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "     #$A:代表输入的句子A，这里是占位符\n",
    "     #:1是type ID，0是第一句话，1是第二句话\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    #模板里写的是字符串 \"[CLS]\"，但计算机只认识数字。\n",
    "    #里是在告诉处理器：“当你在模板里看到 [CLS] 时，请把它替换成词表中对应的 ID（比如 101）；\n",
    "    # 看到 [SEP] 时替换成对应的 ID（比如 102）。”\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5306f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02227d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73fcf0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#指定解码器\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建BPE tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space= False)\n",
    "trainer = trainers.BpeTrainer(vocab_size=25000,special_tokens=[\"<|endoftext|>\"])\n",
    "#法1：流式训练\n",
    "tokenizer.train_from_iterator(get_train_corpus(),trainer=trainer)\n",
    "#法2：文件训练\n",
    "#tokenizer.model = models.BPE()\n",
    "#tokenizer.train([\"wikitext-2.txt\"],trainer=trainer)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "865eb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' test'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's test this tokenizer.\"\n",
    "encoding = tokenizer.encode(sentence)\n",
    "print(encoding.tokens)\n",
    "start, end = encoding.offsets[4]\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ad2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.decode(encoding.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "173a068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "from tokenizers import Regex\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "     [\n",
    "          normalizers.Replace(\"``\", '\"'),\n",
    "          normalizers.Replace(\"''\", '\"'),\n",
    "          normalizers.NFKD(),\n",
    "          normalizers.StripAccents(),\n",
    "          normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "          #会将两个或更多空格替换为一个\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "464d45f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0476f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "     vocab_size=25000,special_tokens=special_tokens,unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_train_corpus(),trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6034397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3b8fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id,sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef2602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "     single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "     pair = \"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "     special_tokens=[(\"<sep>\",sep_token_id),(\"<cls>\",cls_token_id)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae840f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c275eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
