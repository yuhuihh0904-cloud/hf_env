{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f9e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649f5367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16426308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "569db6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdab7c7d06544b43931e1b675da67027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62453\\miniconda3\\envs\\hf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\62453\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810c72386b374f13b45738a4f163ece9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45aa238c3bb94846a7b09031c9be5498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2fbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90faad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {',': 1,\n",
      "             '.': 4,\n",
      "             'Hopefully': 1,\n",
      "             'This': 3,\n",
      "             'ĠCourse': 1,\n",
      "             'ĠFace': 1,\n",
      "             'ĠHugging': 1,\n",
      "             'Ġable': 1,\n",
      "             'Ġabout': 1,\n",
      "             'Ġalgorithms': 1,\n",
      "             'Ġand': 1,\n",
      "             'Ġare': 1,\n",
      "             'Ġbe': 1,\n",
      "             'Ġchapter': 1,\n",
      "             'Ġgenerate': 1,\n",
      "             'Ġhow': 1,\n",
      "             'Ġis': 2,\n",
      "             'Ġsection': 1,\n",
      "             'Ġseveral': 1,\n",
      "             'Ġshows': 1,\n",
      "             'Ġthe': 1,\n",
      "             'Ġthey': 1,\n",
      "             'Ġto': 1,\n",
      "             'Ġtokenization': 1,\n",
      "             'Ġtokenizer': 1,\n",
      "             'Ġtokens': 1,\n",
      "             'Ġtrained': 1,\n",
      "             'Ġunderstand': 1,\n",
      "             'Ġwill': 1,\n",
      "             'Ġyou': 1})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "     word_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "     new_words = [word for word,_ in word_with_offsets]\n",
    "     for word in new_words:\n",
    "          word_freqs[word]+=1\n",
    "pprint(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3d6be08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',',\n",
      " '.',\n",
      " 'C',\n",
      " 'F',\n",
      " 'H',\n",
      " 'T',\n",
      " 'a',\n",
      " 'b',\n",
      " 'c',\n",
      " 'd',\n",
      " 'e',\n",
      " 'f',\n",
      " 'g',\n",
      " 'h',\n",
      " 'i',\n",
      " 'k',\n",
      " 'l',\n",
      " 'm',\n",
      " 'n',\n",
      " 'o',\n",
      " 'p',\n",
      " 'r',\n",
      " 's',\n",
      " 't',\n",
      " 'u',\n",
      " 'v',\n",
      " 'w',\n",
      " 'y',\n",
      " 'z',\n",
      " 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet =[]\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "     for letter in word:\n",
    "          if letter not in alphabet:\n",
    "               alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "pprint(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "985776ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e044d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġthe': ['Ġ', 't', 'h', 'e'], 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'], 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'], 'Ġtokenization': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'], 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ġtokenizer': ['Ġ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'], 'Ġbe': ['Ġ', 'b', 'e'], 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'], 'Ġto': ['Ġ', 't', 'o'], 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ġhow': ['Ġ', 'h', 'o', 'w'], 'Ġthey': ['Ġ', 't', 'h', 'e', 'y'], 'Ġare': ['Ġ', 'a', 'r', 'e'], 'Ġtrained': ['Ġ', 't', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ġtokens': ['Ġ', 't', 'o', 'k', 'e', 'n', 's']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c961d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "     pair_freqs = defaultdict(int)\n",
    "     for word, fred in word_freqs.items():\n",
    "          split = splits[word]\n",
    "          if len(split) == 1:\n",
    "               continue\n",
    "          for i in range(len(split) - 1):\n",
    "               pair = (split[i],split[i+1])\n",
    "               pair_freqs[pair] +=fred\n",
    "     return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11552943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'):3\n",
      "('h', 'i'):3\n",
      "('i', 's'):5\n",
      "('Ġ', 'i'):2\n",
      "('Ġ', 't'):7\n",
      "('t', 'h'):3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i,key in enumerate(pair_freqs.keys()):\n",
    "     print(f\"{key}:{pair_freqs[key]}\")\n",
    "     if i >= 5:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f1fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair =\"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair,freq in pair_freqs.items():\n",
    "     if max_freq is None or max_freq < freq:\n",
    "          best_pair = pair\n",
    "          max_freq = freq\n",
    "\n",
    "print(best_pair,max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "634fe135",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {('Ġ', 't'):\"Ġt\"}\n",
    "vocab.append(\"Ġt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39f09b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a,b,splits):\n",
    "     for word in word_freqs:\n",
    "          split = splits[word]\n",
    "          if len(split) == 1:\n",
    "               continue\n",
    "          i = 0\n",
    "          while i<len(split) -1:\n",
    "               if split[i] == a and split[i+1] == b:\n",
    "                    split =split[:i] + [a+b] + split[i+2:]\n",
    "               else:\n",
    "                    i +=1\n",
    "          splits[word] = split\n",
    "     return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83f67411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"Ġ\", \"t\", splits)\n",
    "print(splits[\"Ġtrained\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "634328dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt'}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52b2d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', \"Ġ't\", 'Th', 'Th', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', \"Ġ't\", 'Ġt']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcbfd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9fbe5bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " 'Ġ',\n",
       " 'i',\n",
       " 's',\n",
       " 'Ġ',\n",
       " 'n',\n",
       " 'o',\n",
       " 't',\n",
       " 'Ġ',\n",
       " 'a',\n",
       " 'Ġt',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
