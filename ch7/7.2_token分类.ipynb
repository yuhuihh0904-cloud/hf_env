{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43461c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets\n",
    "#ner:命名实体识别 pos：词性标注 chunk：分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369569ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fe916",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word,label in zip(words,labels):\n",
    "     full_label = label_names[label]\n",
    "     max_length = max(len(word),len(full_label))\n",
    "     #对齐打印\n",
    "     line1+=word + \" \"* (max_length - len(word) + 1)\n",
    "     line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb966c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][4][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][4][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "     full_label = label_names[label]\n",
    "     max_length = max(len(word),len(full_label))\n",
    "     line1 += word + \" \"*(max_length - len(word) + 1)\n",
    "     line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "print(line1)\n",
    "print(line2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words = True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf212f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lamb 被切分为两个token，这导致输入和标签之间的不匹配\n",
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels,word_ids):\n",
    "     new_labels = []\n",
    "     current_word = None\n",
    "     for word_id in word_ids:\n",
    "          if word_id != current_word:\n",
    "               #新单词的开始\n",
    "               current_word = word_id\n",
    "               label = -100 if word_id is None else labels[word_id]\n",
    "               new_labels.append(label)\n",
    "          elif word_id is None:\n",
    "               #特殊Token\n",
    "               new_labels.append(-100)\n",
    "          else:\n",
    "               #与前一个tokens类型相同的单词\n",
    "               label = labels[word_id]\n",
    "               #如果标签是B-xxx改为I-xxx\n",
    "               if label %2 == 1:\n",
    "                    label += 1\n",
    "               new_labels.append(label)\n",
    "     return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels,word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "     tokenized_inputs = tokenizer(\n",
    "          examples[\"tokens\"],truncation = True,is_split_into_words = True\n",
    "     )\n",
    "     all_labels = examples[\"ner_tags\"]\n",
    "     new_labels = []\n",
    "     for i, labels in enumerate(all_labels):\n",
    "          word_ids = tokenized_inputs.word_ids(i)\n",
    "          new_labels.append(align_labels_with_tokens(labels,word_ids))\n",
    "     tokenized_inputs[\"labels\"] = new_labels\n",
    "     #Hugging Face 的模型在训练时，默认会找一个叫 \"labels\" 的列来计算 Loss。\n",
    "     # 如果不加这一行，模型就不知道正确答案是什么了。\n",
    "     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e04d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "     tokenize_and_align_labels,\n",
    "     batched=True,\n",
    "     #map处理的是一个列表\n",
    "     remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe681fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer= tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "     print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953050cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#它需要字符串形式的标签列表而不是整数\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc314ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "     logits, labels = eval_preds\n",
    "     predictions = np.argmax(logits, axis = -1)\n",
    "\n",
    "     #删除忽略的索引(特殊 tokens )并转换为标签\n",
    "     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "     true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "     ]\n",
    "     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "     return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d151aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048eaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {v:k for k, v in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603356f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "#定义模型\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "     model_checkpoint,\n",
    "     id2label = id2label,\n",
    "     label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be710848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#微调模型\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "     \"bert-finetuned-ner\",\n",
    "     eval_strategy=\"epoch\",\n",
    "     save_strategy= \"epoch\",\n",
    "     learning_rate=2e-5,\n",
    "     num_train_epochs=3,\n",
    "     weight_decay=0.01,\n",
    "     push_to_hub=True,\n",
    "     load_best_model_at_end=True,\n",
    "     save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02349ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "     model = model,\n",
    "     args = args,\n",
    "     train_dataset= tokenized_datasets[\"train\"],\n",
    "     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "     data_collator= data_collator,\n",
    "     compute_metrics=compute_metrics,\n",
    "     tokenizer = tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c322ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"train\"],\n",
    "     shuffle= True,\n",
    "     collate_fn=data_collator,\n",
    "     batch_size=8\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"validation\"],\n",
    "     collate_fn=data_collator,\n",
    "     batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "     model_checkpoint,\n",
    "     id2label = id2label,\n",
    "     label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb79383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6901a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model,optimizer,train_dataloader,eval_dataloader = accelerator.prepare(\n",
    "     model,optimizer,train_dataloader,eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "     \"linear\",\n",
    "     optimizer=optimizer,\n",
    "     num_warmup_steps=0,\n",
    "     num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "     predictions = predictions.detach().cpu().clone().numpy()\n",
    "     labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "     true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "     true_predictions = [\n",
    "          [label_names[p] for (p,l) in zip(prediction,label) if l != -100]\n",
    "          for prediction, label in zip(predictions,labels)\n",
    "     ]\n",
    "     return true_labels,true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "     #训练\n",
    "     model.train()\n",
    "     for batch in train_dataloader:\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)\n",
    "     #评估\n",
    "     model.eval()\n",
    "     for batch in eval_dataloader:\n",
    "          with torch.no_grad():\n",
    "               outputs = model(**batch)\n",
    "          predictions = outputs.logits.argmax(dim = -1)\n",
    "          labels = batch[\"labels\"]\n",
    "\n",
    "          predictions = accelerator.pad_across_processes(predictions,dim =1,pad_index=-100)\n",
    "          labels = accelerator.pad_across_processes(labels,dim=1,pad_index=-100)\n",
    "\n",
    "          predictions_gathered = accelerator.gather(predictions)\n",
    "          labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "          true_predictions, true_labels = postprocess(predictions_gathered,labels_gathered)\n",
    "          metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "     result = metric.compute()\n",
    "     print(\n",
    "          f\"epoch {epoch}:\",\n",
    "          {\n",
    "               key:result[f\"overall_{key}\"]\n",
    "               for key in [\"precision\",\"recall\",\"f1\",\"accuracy\"]\n",
    "          },\n",
    "     )\n",
    "\n",
    "     accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78751fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"yuhuihhh/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "     \"token-classification\",model=model_checkpoint,aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
