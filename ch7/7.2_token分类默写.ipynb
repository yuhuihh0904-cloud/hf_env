{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b71024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据-数据预处理-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e97eaa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62453\\miniconda3\\envs\\hf_env\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "raw_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978ec45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517c04d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8e72aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "#这里忘了\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"],is_split_into_words=True)\n",
    "inputs.tokens()\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b11e3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = inputs.word_ids()\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels,word_ids):\n",
    "     new_labels = []\n",
    "     current_word = None\n",
    "     for word_id in word_ids:\n",
    "          if word_id == None:\n",
    "               new_labels.append(-100)\n",
    "          elif word_id != current_word:\n",
    "               current_word = word_id\n",
    "               label = -100 if word_id == None else labels[word_id]\n",
    "               new_labels.append(label)\n",
    "          else:\n",
    "               #与前一个token是同一个单词\n",
    "               label = labels[word_id]\n",
    "               if label % 2 == 1:\n",
    "                    label += 1\n",
    "               new_labels.append(label)\n",
    "     return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba580e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'BR', '##US', '##SE', '##LS', '1996', '-', '08', '-', '22', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-100, 5, 6, 6, 6, 0, 0, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][2][\"ner_tags\"]\n",
    "inputs = tokenizer(raw_datasets[\"train\"][2][\"tokens\"],is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "print(inputs.tokens())\n",
    "new_labels = align_labels_with_tokens(labels,word_ids)\n",
    "new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ce6f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "     tokenizerd_inputs = tokenizer(\n",
    "          examples[\"tokens\"],\n",
    "          truncation = True,\n",
    "          is_split_into_words = True\n",
    "     )\n",
    "     all_labels = examples[\"ner_tags\"]\n",
    "     new_labels = []\n",
    "     #错误写法 word_ids = tokenized_inputs.word_ids() \n",
    "     # word_ids(i)\n",
    "     for i, labels in enumerate(all_labels):\n",
    "          #必须在.word_ids()这个函数传入批次才能拿到对应的批次\n",
    "          word_ids = tokenizerd_inputs.word_ids(batch_index = i)\n",
    "          new_labels.append(align_labels_with_tokens(labels,word_ids))\n",
    "     tokenizerd_inputs[\"labels\"] = new_labels\n",
    "     return tokenizerd_inputs\n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00c312b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc363844c35463fb6b03deb29d95e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad362eddbaa848a791ac4e2ad69b4645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f53696bd0c4b7a92bff0a6935520cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "     tokenize_and_align_labels,\n",
    "     batched=True,\n",
    "     remove_columns=raw_datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb5698f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "datacollator = DataCollatorForTokenClassification(tokenizer= tokenizer)\n",
    "#bert-base-casd已经写死了，每个单词是对应什么数字，填充符号pad是多少\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33b1bb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "           119,   102],\n",
       "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = datacollator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id映射，模型只认识0123，人只认识B-PER\n",
    "id2label = {i:label for i, label in enumerate(label_names)}\n",
    "label2id = {v:k for k, v  in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "     model_checkpoint,\n",
    "     #这里更换了输出头\n",
    "     num_labels = len(label_names),\n",
    "     id2label = id2label,\n",
    "     label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06fb8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "     logits, labels = eval_preds\n",
    "     predictions = np.argmax(logits,axis=-1)\n",
    "\n",
    "     true_labels = [[label_names[i] for i in label if i != -100] for label in labels]\n",
    "     #这里有可能会把特殊token预测为0，所以必须看prediction和真是标签\n",
    "     true_predictions = [[label_names[i] for (i,j) in zip (prediction,label) if j != -100] for (prediction,label) in zip(predictions,labels)]\n",
    "     all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "     return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf3735eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "     \"bert-finetuned-ner-2\",\n",
    "     eval_strategy=\"epoch\",\n",
    "     save_strategy= \"epoch\",\n",
    "     learning_rate=3e-5,\n",
    "     num_train_epochs=3,\n",
    "     weight_decay=0.01,\n",
    "     load_best_model_at_end=True,\n",
    "     save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680a7f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62453\\AppData\\Local\\Temp\\ipykernel_47948\\2669839198.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 05:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.068862</td>\n",
       "      <td>0.906342</td>\n",
       "      <td>0.933187</td>\n",
       "      <td>0.919569</td>\n",
       "      <td>0.980853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.066043</td>\n",
       "      <td>0.931675</td>\n",
       "      <td>0.945473</td>\n",
       "      <td>0.938523</td>\n",
       "      <td>0.985136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.059633</td>\n",
       "      <td>0.935265</td>\n",
       "      <td>0.950690</td>\n",
       "      <td>0.942914</td>\n",
       "      <td>0.986961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06050381175296815, metrics={'train_runtime': 358.5425, 'train_samples_per_second': 117.484, 'train_steps_per_second': 14.693, 'total_flos': 920771584279074.0, 'train_loss': 0.06050381175296815, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "     args= args,\n",
    "     model= model,\n",
    "     data_collator=datacollator,\n",
    "     train_dataset=tokenized_datasets[\"train\"],\n",
    "     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "     compute_metrics=compute_metrics,\n",
    "     tokenizer = tokenizer\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b5ec574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader实现trainer\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"train\"],\n",
    "     batch_size=8,\n",
    "     shuffle=True,\n",
    "     num_workers=4,\n",
    "     collate_fn=datacollator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"validation\"],\n",
    "     batch_size=8,\n",
    "     collate_fn=datacollator,\n",
    "     num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da9258c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "     model_checkpoint,\n",
    "     id2label = id2label,\n",
    "     label2id = label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c8efe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr=3e-5,weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c655735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model,train_dataloader,eval_dataloader,optimizer = accelerator.prepare(\n",
    "     model,train_dataloader,eval_dataloader,optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47184dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_train_update_steps = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_train_update_steps\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "     \"linear\",\n",
    "     optimizer= optimizer,\n",
    "     num_warmup_steps=0,\n",
    "     num_training_steps=num_training_steps,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98eb66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprogress(predictions,labels):\n",
    "     #在评估阶段，detach()是从计算图里分离出来，以后他的计算不需要反向传播求导了\n",
    "     predictions.detach().cpu().clone().numpy()\n",
    "     labels.detach().cpu().cpu().numpy()\n",
    "\n",
    "     true_labels = [[label_names[i] for i in label if i != -100] for label in labels]\n",
    "     true_predictions = [[label_names[i] for (i,j) in zip(prediction,label) if j != -100] for (prediction,label) in zip(predictions,labels)]\n",
    "     return true_predictions,true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d4b1de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18f48c13cf041be8aea9a8714731f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0: {'precision': 0.9090165275732286, 'recall': 0.9348704140020195, 'f1': 0.9217622168754668, 'accuracy': 0.983781715429446}\n",
      "epoch1: {'precision': 0.9287249875971556, 'recall': 0.9451363177381353, 'f1': 0.9368587872216199, 'accuracy': 0.9851209748631307}\n",
      "epoch2: {'precision': 0.9270781893004115, 'recall': 0.9478290138000673, 'f1': 0.937338770075726, 'accuracy': 0.9859304173779949}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "     model.train()\n",
    "     for batch in train_dataloader:\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)\n",
    "     model.eval()\n",
    "     for batch in eval_dataloader:\n",
    "          with torch.no_grad():\n",
    "               outputs = model(**batch)\n",
    "          predictions = outputs.logits.argmax(dim = -1)\n",
    "          labels = batch[\"labels\"]\n",
    "          true_predictions, true_labels = postprogress(predictions,labels)\n",
    "          metric.add_batch(predictions=true_predictions,references=true_labels)\n",
    "     result = metric.compute()\n",
    "     print(\n",
    "         f\"epoch{epoch}:\",\n",
    "         {\n",
    "              key:result[f\"overall_{key}\"]\n",
    "              for key in [\"precision\",\"recall\",\"f1\",\"accuracy\"]\n",
    "         }\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
