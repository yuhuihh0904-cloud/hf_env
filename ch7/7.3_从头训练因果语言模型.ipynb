{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9d82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "     for keyword in keywords:\n",
    "          if keyword in string:\n",
    "               return True\n",
    "     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7174f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\",\"sklearn\",\"matplotlib\",\"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "print(\n",
    "     any_keyword_in_string(example_1,filters),any_keyword_in_string(example_2,filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "#defaultdictæ‹¬å·é‡Œçš„listæ˜¯ç”¨æ¥ç”Ÿæˆé»˜è®¤å€¼çš„\n",
    "#å½“æˆ‘è¦è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„keyæ—¶ï¼Œè°ƒç”¨list()è¿™ä¸ªå‡½æ•°ï¼ŒæŠŠç”Ÿæˆçš„ç©ºåˆ—è¡¨[]ä½œä¸ºè¿™ä¸ªkeyçš„åˆå§‹å€¼\n",
    "def filter_streaming_dataset(dataset,filter,max_samples=100000):\n",
    "     filtered_dict = defaultdict(list)\n",
    "     total = 0\n",
    "     kept = 0\n",
    "     for sample in tqdm(iter(dataset)):\n",
    "          total+=1\n",
    "          #sampleæ˜¯å•æ¡æ•°æ®ä¹Ÿå°±æ˜¯è¡Œå¼æ•°æ®\n",
    "          #æˆ‘ä»¬éœ€è¦æŠŠè¡Œå¼æ•°æ®è½¬æ¢ä¸ºåˆ—å¼æ•°æ®ï¼Œ\n",
    "          # è¿™æ ·æ‰èƒ½ç”¨dataset.from_dictè½¬æ¢æ ‡å‡†çš„Datasetå¯¹è±¡\n",
    "          if any_keyword_in_string(sample[\"content\"],filters):\n",
    "               for k,v in sample.items():\n",
    "                    filtered_dict[k].append(v)\n",
    "               kept+=1\n",
    "               if kept >= max_samples:\n",
    "                    print(f\"ğŸ›‘ å·²æ”¶é›†åˆ° {max_samples} æ¡æ•°æ®ï¼Œæå‰åœæ­¢ã€‚\")\n",
    "                    break\n",
    "               \n",
    "     print(f\"{len(filtered_dict['content']) / total:.2%} of data after filtering\")\n",
    "     return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c440efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¸‹è½½éªŒè¯é›†ï¼ˆèµ°å›½å†…é•œåƒï¼‰...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776bc4321cd84d8e80320da06e02a3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19dd2829bfc46fd85df4270e137e2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset,DatasetDict\n",
    "\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "print(\"æ­£åœ¨ä¸‹è½½éªŒè¯é›†ï¼ˆèµ°å›½å†…é•œåƒï¼‰...\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "     {\n",
    "          \"train\":ds_train,\n",
    "          \"valid\":ds_valid\n",
    "     }\n",
    ")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c31fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME:kmike/scikit-learn\n",
      "PATH:sklearn/utils/__init__.py\n",
      "COPIES:3\n",
      "SIZE:10094\n",
      "CONTENT:\"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm\n",
      "LICENSE:bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "     print(f\"{key.upper()}:{raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "909a7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "     raw_datasets[\"train\"][:2][\"content\"],\n",
    "     truncation = True,\n",
    "     max_length = context_length,\n",
    "     return_overflowing_tokens = True,\n",
    "     return_length = True\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3dff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping']\n"
     ]
    }
   ],
   "source": [
    "print(list(outputs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd71fad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54812d8f073b4957b77bb2ce310af552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/606720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82228fc668904ab2acc3748c0f2b2d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16702061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element,tokenizer,context_length):\n",
    "     outputs = tokenizer(\n",
    "          element[\"content\"],\n",
    "          truncation = True,\n",
    "          return_overflowing_tokens = True,\n",
    "          max_length = context_length,\n",
    "          return_length = True\n",
    "     )\n",
    "     input_batch = []\n",
    "     for length, input_ids in zip(outputs[\"length\"],outputs[\"input_ids\"]):\n",
    "          if length == context_length:\n",
    "               input_batch.append(input_ids)\n",
    "     return {\"input_ids\":input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "     tokenize,\n",
    "     remove_columns= raw_datasets[\"train\"].column_names,\n",
    "     batched=5000,\n",
    "     num_proc=8,\n",
    "     fn_kwargs={\"tokenizer\":tokenizer,\"context_length\":context_length}\n",
    "     \n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d57eb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆå§‹åŒ–æ–°æ¨¡å‹\n",
    "\n",
    "from transformers import AutoTokenizer,GPT2LMHeadModel,AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "     \"gpt2\",\n",
    "     vocab_size = len(tokenizer),\n",
    "     n_ctx = context_length,\n",
    "     bos_token_id = tokenizer.bos_token_id,\n",
    "     eos_token_id = tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6522caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size : 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size : {model_size / 1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d2c9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed72daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_idsshape:torch.Size([5, 128])\n",
      "attention_maskshape:torch.Size([5, 128])\n",
      "labelsshape:torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "     print(f\"{key}shape:{out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3454752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ef200dd02b41328065fbfb12a3ce1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61cee6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62453\\AppData\\Local\\Temp\\ipykernel_50116\\656557464.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 1:40:57, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.200800</td>\n",
       "      <td>3.074660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.561300</td>\n",
       "      <td>2.484902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.303300</td>\n",
       "      <td>2.241182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.243300</td>\n",
       "      <td>2.175087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=2.9978749389648436, metrics={'train_runtime': 6060.4271, 'train_samples_per_second': 84.482, 'train_steps_per_second': 0.33, 'total_flos': 3.3445380096e+16, 'train_loss': 2.9978749389648436, 'epoch': 0.030654893910117935})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "     output_dir=\"codeparrot-ds\",\n",
    "     per_device_train_batch_size=8,\n",
    "     max_steps=2000,\n",
    "     dataloader_pin_memory=True,\n",
    "     per_device_eval_batch_size=16,\n",
    "     eval_strategy=\"steps\",\n",
    "     logging_steps=50,\n",
    "     eval_steps=500,\n",
    "     gradient_accumulation_steps=32,\n",
    "     num_train_epochs=1,\n",
    "     weight_decay=0.1,\n",
    "     warmup_steps=200,\n",
    "     lr_scheduler_type=\"cosine\",\n",
    "     learning_rate=5e-4,\n",
    "     save_steps=500,\n",
    "     fp16=True,\n",
    "     save_total_limit=1,\n",
    "     load_best_model_at_end=True,\n",
    "     dataloader_num_workers=0\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "     model=model,\n",
    "     args= args,\n",
    "     data_collator=data_collator,\n",
    "     tokenizer = tokenizer,\n",
    "     train_dataset=tokenized_datasets[\"train\"],\n",
    "     eval_dataset=tokenized_datasets[\"valid\"]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "494cc564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0ccfd925724aff843596e006f9702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893a5e58939847dfbe0835e8de7b0408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac21f6e4fca84bc8b60bc90cc2fc0a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/b9/a1/b9a1df1d00c6e128245b95c00222135e0e3117ffdac06cf01f3ee0ca1e684fd0/85a5dcfdf3d89508b9d8e9551ee9ac3eaf980a178eb277a2db3bd771dd69f62a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20260103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260103T133603Z&X-Amz-Expires=86400&X-Amz-Signature=54874e98fd160f3c5d17bdadca12f035a924e9c45335bdfc89746ceb7c955f70&X-Amz-SignedHeaders=host&partNumber=1&uploadId=7.kwZOHH2bP_6.UqVe.k7B7DtbIW58BnGraHK8IWoQccVhwhiMMWcOdPKr9ZX1tHtSA85N7iqf_Q77eUJP3YIkyKkx4VJN6EiS0taheBhvC2C3FB.z9l1IY7iStOdt57&x-id=UploadPart (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\"), '(Request ID: f1272e93-421c-440c-8788-6cfa3a6dc2c5)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/b9/a1/b9a1df1d00c6e128245b95c00222135e0e3117ffdac06cf01f3ee0ca1e684fd0/85a5dcfdf3d89508b9d8e9551ee9ac3eaf980a178eb277a2db3bd771dd69f62a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20260103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260103T133603Z&X-Amz-Expires=86400&X-Amz-Signature=54874e98fd160f3c5d17bdadca12f035a924e9c45335bdfc89746ceb7c955f70&X-Amz-SignedHeaders=host&partNumber=1&uploadId=7.kwZOHH2bP_6.UqVe.k7B7DtbIW58BnGraHK8IWoQccVhwhiMMWcOdPKr9ZX1tHtSA85N7iqf_Q77eUJP3YIkyKkx4VJN6EiS0taheBhvC2C3FB.z9l1IY7iStOdt57&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yuhuihhh/codeparrot-ds/commit/fc91dc61dba7f2add2f4cdc0b99da261421dba32', commit_message='End of training', commit_description='', oid='fc91dc61dba7f2add2f4cdc0b99da261421dba32', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yuhuihhh/codeparrot-ds', endpoint='https://huggingface.co', repo_type='model', repo_id='yuhuihhh/codeparrot-ds'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10bee52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2131c4b52b73447ab20cbc27835c16f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62453\\miniconda3\\envs\\hf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\62453\\.cache\\huggingface\\hub\\models--yuhuihhh--codeparrot-ds. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7258e8bc46c243a4a7af3ace63a8a466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19567cc4559c46db97bebeb03de65dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7447e5164ff84f5a9cb9ce935f2a607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c79876a354b402987892093d284204d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7854bcc522cb40fd963614271457ac3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3089aa357abf4d41bb8b1fedad7cf0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc576ae7904441ec851abaf59ddd19bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/497 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "     \"text-generation\",model = \"yuhuihhh/codeparrot-ds\",device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c63ccd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# Plot a scatter plot using x and y\n",
      "xx, yy = np.meshgrid(x, y)\n",
      "\n",
      "# Plot also the training points\n",
      "X, y = np.meshgrid(x, y)\n",
      "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
      "plt.axis('tight')\n",
      "plt.title(\"scikit-learn support with n_features + 1 features\")\n",
      "plt.scatter(X[:, 0, 0], X[:, 1], color='roston_r', c=y, c=y, cmap=y, cmap=y, cmap=y, cmap=y, cmap=y_ cmap=y, cmap=c, cmap=y, cmap=y_weights=np.5, cmap=y_path.astype(X_weights=y_path.T, cmap=y, cmap=y_path.3, cmap=y_path.ravel(), cmap=y_path.T)\n",
      "plt.T, cmap=colors, cmap=y_path.T, cmap=y_path.T, cmap=y_path.5)\n",
      "plt.astype(y_path.T)\n",
      "plt.astype(X_path.T,\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# Create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# Plot a scatter plot using x and y\n",
    "\"\"\"\n",
    "print(pipe(txt,num_return_sequences = 1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebe8a66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Create some data\n",
      "x = np.random.randn(100)\n",
      "y = np.random.randn(100)\n",
      "\n",
      "# Create dataframe from x and y\n",
      "x = x.reshape(2, 1)\n",
      "y = y.reshape(2, 1)\n",
      "\n",
      "# Create a dataframe of the test\n",
      "x = x.reshape(2, 1)\n",
      "y = y.reshape(2, 1)\n",
      "\n",
      "# Create an array of the same size as the first dimension\n",
      "x = x.reshape(2, 1)\n",
      "y = y.reshape(2, 1)\n",
      "\n",
      "# Create a dataset\n",
      "X = np. dataset\n",
      "\n",
      "X = np.data\n",
      "x\n",
      "\n",
      "\n",
      "X = np.data\n",
      "y_train(np.data\n",
      "\n",
      "#\n",
      "x = np.target\n",
      "\n",
      "y = np.target = np.target\n",
      "y = x_train(X_train_train(X[x[:10\n",
      "\n",
      "y[0]\n",
      "X[:10:]\n",
      "X[:, 0.train_test\n",
      "y]\n",
      "X[:10]\n",
      "\n",
      "y[10:]\n",
      "y[:10:]\n",
      "x_train[:10:]\n",
      "y_train + y_train_train_train[:10:]\n",
      "y_test_train[:10:]\n",
      "y[:10:]\n",
      "X_train[20:]\n",
      "\n",
      "# y_train:]\n",
      "\n",
      "\n",
      "# y_\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# Create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# Create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
