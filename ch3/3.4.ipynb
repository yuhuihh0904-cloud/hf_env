{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db38416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer,DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\",\"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "     return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation = True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\",\"sentence2\",\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d54b7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([16]),\n",
       " 'input_ids': torch.Size([16, 86]),\n",
       " 'token_type_ids': torch.Size([16, 86]),\n",
       " 'attention_mask': torch.Size([16, 86])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"train\"],shuffle=True,batch_size=16,collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "     tokenized_datasets[\"validation\"],batch_size=8,collate_fn=data_collator\n",
    ")\n",
    "for batch in train_dataloader:\n",
    "     break\n",
    "{k:v.shape for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "482ae899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7541, grad_fn=<NllLossBackward0>) torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = 2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538c41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(),lr = 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0194af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "     \"linear\",\n",
    "     optimizer=optimizer,\n",
    "     num_warmup_steps=0,\n",
    "     num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea90ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80cfc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911d571f98474931849d20b536070741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "     for batch in train_dataloader:\n",
    "          batch = {k : v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7d88dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8333333333333334, 'f1': 0.8843537414965986}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\",\"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "     batch = {k: v.to(device) for k,v in batch.items()}\n",
    "     with torch.no_grad():\n",
    "          outputs = model(**batch)\n",
    "\n",
    "     logits = outputs.logits\n",
    "     predictions = torch.argmax(logits,dim = -1)\n",
    "     metric.add_batch(predictions=predictions,references=batch[\"labels\"])\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f7fc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3ce7e17729470c8289034ab985a52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels = 2)\n",
    "optimizer = AdamW(model.parameters(),lr = 3e-5)\n",
    "\n",
    "train_dl, eval_dl, model,optimizer = accelerator.prepare(\n",
    "     train_dataloader,eval_dataloader,model,optimizer\n",
    ")\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "     \"linear\",\n",
    "     optimizer=optimizer,\n",
    "     num_warmup_steps=0,\n",
    "     num_training_steps= num_training_steps,\n",
    "\n",
    ")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "     for batch in train_dl:\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          accelerator.backward(loss)\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
